# End-to-End Data Engineering Pipeline Using Airflow, AWS Glue, Redshift & PySpark

This project demonstrates a production-grade data engineering pipeline orchestrated using **Apache Airflow**, with scalable data processing using **AWS Glue (PySpark)** and downstream loading into **Amazon Redshift** using staging + **SCD Type 2** merge logic.

It showcases key enterprise data engineering capabilities:

- Event-driven orchestration using Airflow  
- Schema discovery using Glue Crawler  
- Automated Data Quality Checks (Deequ)  
- Scalable PySpark transformation jobs  
- Redshift COPY into staging  
- SCD Type 2 merging into final tables  
- End-to-end monitoring & reliability

---

## ðŸ“Œ Architecture Overview

```text
S3 (Landing Zone)
      â†“   (S3PrefixSensor)
Airflow DAG Trigger
      â†“
AWS Glue Crawler â†’ Updates Glue Data Catalog
      â†“
AWS Glue Data Quality Job (PySpark + Deequ)
      â†“   (Airflow Python validation)
AWS Glue Transformation Job (PySpark)
      â†“
S3 (Processed Zone)
      â†“
Redshift COPY â†’ Staging Table
      â†“
Redshift MERGE â†’ Target Table (SCD Type 2)
